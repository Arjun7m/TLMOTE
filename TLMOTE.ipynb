{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TLMOTE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "fn4be11DIGQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fnAAQbkZCYp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data/V1.4_Training.csv')\n",
        "print(train_df.head)\n",
        "train_df.dropna(inplace=True)\n",
        "print(train_df.shape)\n",
        "sugg_samples = []\n",
        "print(train_df.size)\n",
        "for i in range(len(train_df)):\n",
        "    if(train_df.iloc[i]['Tag']==1):\n",
        "        sugg_samples.append(train_df.loc[i]['Sentence'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_of_topic_words = 100\n",
        "num_of_ngrams = 15000"
      ],
      "metadata": {
        "id": "oYdWRraipWT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extraction of all n-grams for n=5"
      ],
      "metadata": {
        "id": "9zVUPLzAbIKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(sugg_samples)"
      ],
      "metadata": {
        "id": "sYI4XkzWbgK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "stop_words = set(stopwords.words('english')) "
      ],
      "metadata": {
        "id": "br_CL8Y3hDnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "textp = ' '.join(sugg_samples)\n",
        "textp = textp.lower()\n",
        "textp = textp.translate(textp.maketrans('','', string.punctuation))\n",
        "textp = word_tokenize(textp)"
      ],
      "metadata": {
        "id": "oK8WcMvEhRRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngrams = lambda a, n: zip(*[a[i:] for i in range(n)])"
      ],
      "metadata": {
        "id": "w0tw1PMPhSot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "mc_sugg = Counter(ngrams(textp, 5)).most_common(num_of_ngrams)\n",
        "mc_sugg"
      ],
      "metadata": {
        "id": "eCSwXD0ThUsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic Modeling"
      ],
      "metadata": {
        "id": "wEQgiHi3hbsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "VAi7l80zhYQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "import nltk.stem as stemmer\n",
        "import numpy as np\n",
        "import collections"
      ],
      "metadata": {
        "id": "qdJhkGhshfP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(2018)\n",
        "import nltk"
      ],
      "metadata": {
        "id": "27V8BkA0iyPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "DYZap1a4j5j-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_stemming(text):\n",
        "    stemmer = SnowballStemmer('english')\n",
        "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
        "def preprocess(text):\n",
        "    result = []\n",
        "    for token in gensim.utils.simple_preprocess(text):\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3 and token!=\"https\":\n",
        "            result.append(lemmatize_stemming(token))\n",
        "    return result"
      ],
      "metadata": {
        "id": "U63lICA9i1yK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def string2cnov(text,dict):\n",
        "            \n",
        "    val=0\n",
        "    pres=\"\"\n",
        "    flag=False\n",
        "    for curr in range(0,len(text)-1):\n",
        "        \n",
        "        #print (text[curr] +\" \")\n",
        "        #print(pres+\" \")\n",
        "        if text[curr]=='*':\n",
        "          val=float(pres)\n",
        "          pres=\"\"\n",
        "        elif text[curr]=='\"' and flag==False :\n",
        "          \n",
        "          flag=True\n",
        "          \n",
        "          curr=curr+1\n",
        "          ster=\"\"\n",
        "          while (text[curr]!='\"') :\n",
        "                ster=ster+text[curr]\n",
        "                curr=curr+1\n",
        "          curr=curr+3\n",
        "          filler=0\n",
        "          if (ster!=\".\"):\n",
        "              if ster in dict.keys():\n",
        "                  filler=dict[ster]\n",
        "              dict[ster]=filler+val\n",
        "              if (curr>=len(text)):\n",
        "                break\n",
        "          #print(text[curr])\n",
        "          \n",
        "          pres=\"\"\n",
        "          val=0\n",
        "        elif text[curr]=='\"' and flag==True :\n",
        "            flag=False\n",
        "        elif text[curr]=='.' or (text[curr]>='0' and text[curr]<='9'):\n",
        "            pres=pres+text[curr]\n",
        "    return dict"
      ],
      "metadata": {
        "id": "pJxnT_mAi5No"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tdictr = {}"
      ],
      "metadata": {
        "id": "bMvXwT5tmSyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_docs = [preprocess(text) for text in sugg_samples]\n",
        "# processed_docs[:10]"
      ],
      "metadata": {
        "id": "bxwCikwfi-BS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
        "count = 0\n",
        "for k, v in dictionary.iteritems():\n",
        "    # print(k, v)\n",
        "    count += 1\n",
        "    if count > 50:\n",
        "        break\n"
      ],
      "metadata": {
        "id": "jwCJHvXbjnx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
        "bow_corpus[len(bow_corpus)-1]"
      ],
      "metadata": {
        "id": "0pbrbWH8l4mP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim import corpora, models\n",
        "tfidf = models.TfidfModel(bow_corpus)\n",
        "corpus_tfidf = tfidf[bow_corpus]\n",
        "from pprint import pprint\n",
        "for doc in corpus_tfidf:\n",
        "    pprint(doc)\n",
        "    break"
      ],
      "metadata": {
        "id": "xVxI9OXfl-jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=100, id2word=dictionary, passes=2, workers=2)"
      ],
      "metadata": {
        "id": "EfDVb4nzmCsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictr={}\n",
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print('Topic: {} \\nWords: {}'.format(idx, topic))\n",
        "    d=string2cnov(topic,dictr)\n",
        "    t=string2cnov(topic,tdictr)"
      ],
      "metadata": {
        "id": "0FZ7EFUzmFMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newd = dict(Counter(dictr).most_common(num_of_topic_words))\n",
        "values = list(newd.values())"
      ],
      "metadata": {
        "id": "tMD2rOrRmIMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_words = set(newd.keys())"
      ],
      "metadata": {
        "id": "FYJKjNtUmhff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_words"
      ],
      "metadata": {
        "id": "OzkaKdHsrUx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extraction of N-Grams with atleast one word among the most common keywords"
      ],
      "metadata": {
        "id": "d_GyOxSupwKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ngrams_with_topics(mc_sugg, topic_words):\n",
        "    filtered = []\n",
        "    for ng in mc_sugg:\n",
        "        flag = 0\n",
        "        for word in ng[0]:\n",
        "            if word in topic_words:\n",
        "                flag = 1\n",
        "        if flag:\n",
        "            filtered.append(ng)\n",
        "    return filtered"
      ],
      "metadata": {
        "id": "e7vHwI_jmnZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_mc_sugg = ngrams_with_topics(mc_sugg, topic_words)\n",
        "filtered_mc_sugg"
      ],
      "metadata": {
        "id": "7cIq7f96rDxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_mc_sugg = filtered_mc_sugg[:4330]"
      ],
      "metadata": {
        "id": "p0zgocF-Q3yu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing for training language models"
      ],
      "metadata": {
        "id": "OH3K6NRPsMGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(df):\n",
        "    inp = df.values\n",
        "    for i in range(len(inp)):\n",
        "        s = inp[i]\n",
        "        sl = s.lower()\n",
        "        sl = result = sl.translate(sl.maketrans('','', string.punctuation))\n",
        "        tokens = word_tokenize(sl)\n",
        "        #tokens = [w for w in tokens if not w in stop_words]\n",
        "        inp[i] = tokens\n",
        "    return inp\n",
        "\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data/V1.4_Training.csv')\n",
        "train_df.dropna(inplace=True)\n",
        "train_df = train_df[train_df['Tag']==1]['Sentence']\n",
        "textall = preprocess(train_df)"
      ],
      "metadata": {
        "id": "C8pgVlw3xemz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "en_model = {}\n",
        "f = open('/content/drive/My Drive/data/glove.6B.100d.txt', encoding='utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    en_model[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(en_model))"
      ],
      "metadata": {
        "id": "wPlDOXQ2x53j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in textall:\n",
        "    for w in t:\n",
        "        if w in string.punctuation:\n",
        "            t.remove(w)"
      ],
      "metadata": {
        "id": "NWMl66A_yFsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words=[]\n",
        "keys=[]\n",
        "i=1\n",
        "print('starting')\n",
        "for k, tsent in enumerate(textall):\n",
        "    if (k%1000 == 0):\n",
        "        print ('k', k, k/textall.size)\n",
        "    for j, word in enumerate(tsent):\n",
        "        #if (j%100 == 0):\n",
        "            #print ('j', j)\n",
        "        if word not in words:\n",
        "            words.append(word)\n",
        "            keys.append(i)\n",
        "            i=i+1\n",
        "print ('done!')"
      ],
      "metadata": {
        "id": "tQHwXFRayHF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_len = len(words)\n",
        "print (embed_len)\n",
        "\n",
        "word2id = dict(zip(words, keys))\n",
        "id2word = dict(zip(keys, words))\n",
        "embeddings = np.zeros((embed_len+1,len(en_model['test'])))  # num_words * 100 (word vec len)\n",
        "print('starting')\n",
        "for i, w in enumerate(words):\n",
        "    try:\n",
        "        vec = en_model[w]\n",
        "    except KeyError:\n",
        "        pass\n",
        "    embeddings[word2id[w]]=vec\n",
        "print('done')"
      ],
      "metadata": {
        "id": "xv99ZhryyJgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ngrams_lang_inp(arr):\n",
        "    inp = []\n",
        "    curr = []\n",
        "    for s in arr:\n",
        "        i=0\n",
        "        while(i+5<len(s)):\n",
        "            ngram = list(s[i:i+6])\n",
        "            idngram = [word2id[w] for w in ngram]\n",
        "            if i+6<len(s):\n",
        "                nextword = word2id[s[i+6]]   \n",
        "            else:\n",
        "                break\n",
        "            curr = [idngram, nextword]\n",
        "            inp.append(curr)\n",
        "            curr = []\n",
        "            i=i+1\n",
        "        \n",
        "    return inp"
      ],
      "metadata": {
        "id": "Q-yyzQc4q8oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sugg_text = []\n",
        "sugg_len = len(sugg_samples)\n",
        "print (sugg_len)\n",
        "print ('starting')\n",
        "for i in range(sugg_len):\n",
        "    s = sugg_samples[i]\n",
        "    sl = s.lower()\n",
        "    sl = result = sl.translate(sl.maketrans('','', string.punctuation))\n",
        "    tokens = word_tokenize(sl)\n",
        "    sugg_text.append(tokens)\n",
        "print ('finishing')\n",
        "sugg_lang_seq = ngrams_lang_inp(sugg_text)\n",
        "train_xp = np.array([s[0] for s in sugg_lang_seq])\n",
        "train_yp = np.array([s[1] for s in sugg_lang_seq])\n",
        "\n",
        "print('done')"
      ],
      "metadata": {
        "id": "WqN-Mlz6rBzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Language Models"
      ],
      "metadata": {
        "id": "BoSmc7bYsXFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Dense, Input, Bidirectional\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "modelp = Sequential()\n",
        "modelp.add(Embedding(embeddings.shape[0], output_dim = 100,\n",
        "                     weights = [embeddings], input_length=6, trainable = True))\n",
        "modelp.add(Bidirectional(LSTM(256, activation = 'relu')))\n",
        "modelp.add(Dense(1024, activation = 'relu'))\n",
        "modelp.add(Dense(embeddings.shape[0], activation = 'softmax'))\n",
        "\n",
        "modelp.compile(optimizer = 'Adam',\n",
        "               loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "IofWQVVgrZi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelp.fit(train_xp, train_yp, batch_size = 64, epochs=20, verbose=True)"
      ],
      "metadata": {
        "id": "k3e1KW3MrlaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelp.save('/content/drive/My Drive/data/sugg_model6')"
      ],
      "metadata": {
        "id": "toJVHvp5rpRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Language Models"
      ],
      "metadata": {
        "id": "rBn99MQ3sfm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "model4 = keras.models.load_model('/content/drive/My Drive/data/sugg_model4')\n",
        "model5 = keras.models.load_model('/content/drive/My Drive/data/sugg_model5')\n",
        "model6 = keras.models.load_model('/content/drive/My Drive/data/sugg_model6')"
      ],
      "metadata": {
        "id": "XDTZ_4Lmywab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation of synthetic datapoints using the three language models"
      ],
      "metadata": {
        "id": "MCsjg7_v2yRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Randomized output of 3 language models for generation of synthetic sample\n",
        "import numpy as np\n",
        "sugg_generated = []\n",
        "for sug in filtered_mc_sugg:\n",
        "    ngram_gen = sug[0] #('', '', '', '', '')\n",
        "    idngram = []\n",
        "    for p in ngram_gen:\n",
        "        idngram.append(word2id[p])\n",
        "    #[1, 2, 3, 4, 5]\n",
        "    ran_len = np.random.randint(20, 30)\n",
        "    sentence = [id2word[i] for i in idngram]\n",
        "    curr5 = idngram[:5]\n",
        "    curr4 = idngram[1:5]\n",
        "    curr6 = idngram[:5]\n",
        "    for l in range(ran_len):\n",
        "        inp4 = np.array([curr4, [2, 3, 4, 5]])\n",
        "        out4 = model4.predict(inp4)\n",
        "        i4 = np.argmax(out4[0])\n",
        "        max4 = out4[0][i4]\n",
        "        inp5 = np.array([curr5, [2, 3, 4, 5, 6]])\n",
        "        out5 = model5.predict(inp5)\n",
        "        i5 = np.argmax(out5[0])\n",
        "        max5 = out5[0][i5]\n",
        "        if l != 0:\n",
        "          inp6 = np.array([curr6, [2, 3, 4, 5, 6, 7]])\n",
        "          out6 = model6.predict(inp6)\n",
        "          i6 = np.argmax(out6[0])\n",
        "          max6 = out6[0][i6]\n",
        "\n",
        "        if l != 0:\n",
        "          z = np.random.randint(0, 2)\n",
        "\n",
        "        else:\n",
        "          z = np.random.randint(0, 1)\n",
        "\n",
        "        if z==0:\n",
        "          p = i4\n",
        "          wordp = id2word[p]\n",
        "          sentence.append(wordp)\n",
        "          curr4 = np.delete(curr4, 0)\n",
        "          curr4 = np.append(curr4, p)\n",
        "          curr5 = np.delete(curr5, 0)\n",
        "          curr5 = np.append(curr5, p)\n",
        "          if l != 0:\n",
        "            curr6 = np.delete(curr6, 0)\n",
        "            curr6 = np.append(curr6, p)\n",
        "\n",
        "          else:\n",
        "            curr6 = np.append(curr6, p)\n",
        "\n",
        "        elif z==1:\n",
        "          p = i5\n",
        "          wordp = id2word[p]\n",
        "          sentence.append(wordp)\n",
        "          curr4 = np.delete(curr4, 0)\n",
        "          curr4 = np.append(curr4, p)\n",
        "          curr5 = np.delete(curr5, 0)\n",
        "          curr5 = np.append(curr5, p)\n",
        "          if l != 0:\n",
        "            curr6 = np.delete(curr6, 0)\n",
        "            curr6 = np.append(curr6, p)\n",
        "\n",
        "          else:\n",
        "            curr6 = np.append(curr6, p)\n",
        "\n",
        "        elif z==2:\n",
        "          p = i6\n",
        "          wordp = id2word[p]\n",
        "          sentence.append(wordp)\n",
        "          curr4 = np.delete(curr4, 0)\n",
        "          curr4 = np.append(curr4, p)\n",
        "          curr5 = np.delete(curr5, 0)\n",
        "          curr5 = np.append(curr5, p)\n",
        "          curr6 = np.delete(curr6, 0)\n",
        "          curr6 = np.append(curr6, p)\n",
        "    sugg_generated.append(sentence)"
      ],
      "metadata": {
        "id": "lpygogUQcNum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_list = []\n",
        "for l in sugg_generated:\n",
        "  sentence = \" \".join(l)\n",
        "  final_list.append(sentence)"
      ],
      "metadata": {
        "id": "PdnA7xxldoFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_list"
      ],
      "metadata": {
        "id": "jAq7S-zuQri8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Augmenting the dataset with the synthetic samples"
      ],
      "metadata": {
        "id": "I9IRUM3z2-V7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(final_list, columns=[\"Sentence\"])\n",
        "df['Tag'] = 1"
      ],
      "metadata": {
        "id": "K_YE9OckQtB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data/V1.4_Training.csv')\n",
        "result = train_df.append(df)"
      ],
      "metadata": {
        "id": "FLnV1Y8xReYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.to_csv('/content/drive/MyDrive/Colab Notebooks/Data/Augmented_SM_Dataset.csv', index = False)"
      ],
      "metadata": {
        "id": "CPzw6OxeS2mE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}